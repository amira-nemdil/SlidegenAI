{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb21cfa",
   "metadata": {},
   "source": [
    "\n",
    "# Automated Slide Generation from Research Papers\n",
    "*Final Study Project Notebook*\n",
    "\n",
    "**Goal:** Automatically generate PowerPoint slides from research papers using the arXiv dataset, local NLP models, and Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6761ba6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "import re\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6ccbc",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust path as needed\n",
    "DATA_PATH = \"C:/Users/amira/Downloads/datasets SlidegenAI/arxiv-dataset/arxiv-dataset/train.txt\"\n",
    "\n",
    "def load_jsonl_lines(filepath, n=1):\n",
    "    \"\"\"Load the first n lines from a .txt file (JSON objects per line)\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load 1 sample for demo (change n for more)\n",
    "samples = load_jsonl_lines(DATA_PATH, n=1)\n",
    "print(\"Sample keys:\", samples[0].keys())\n",
    "print(\"\\nArticle preview:\\n\", samples[0]['article'][:1000])\n",
    "print(\"\\nAbstract preview:\\n\", samples[0]['abstract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b96681",
   "metadata": {},
   "source": [
    "## 3. Article Section Splitting (Heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sections(text):\n",
    "    \"\"\"\n",
    "    Split article into sections using common section headers.\n",
    "    Returns a list of (section_title, section_text) tuples.\n",
    "    \"\"\"\n",
    "    # Typical section headers in arXiv papers\n",
    "    headers = [\n",
    "        r'(?i)\\n\\s*abstract\\s*\\n',\n",
    "        r'(?i)\\n\\s*introduction\\s*\\n',\n",
    "        r'(?i)\\n\\s*related work[s]?\\s*\\n',\n",
    "        r'(?i)\\n\\s*method[s]?(ology)?\\s*\\n',\n",
    "        r'(?i)\\n\\s*experiment[s]?\\s*\\n',\n",
    "        r'(?i)\\n\\s*results\\s*\\n',\n",
    "        r'(?i)\\n\\s*discussion[s]?\\s*\\n',\n",
    "        r'(?i)\\n\\s*conclusion[s]?\\s*\\n',\n",
    "        r'(?i)\\n\\s*references\\s*\\n'\n",
    "    ]\n",
    "    pattern = \"|\".join(headers)\n",
    "    # Split, keep the headers\n",
    "    splits = re.split(pattern, text)\n",
    "    # Find where the headers occur for titles\n",
    "    headers_found = re.findall(pattern, text)\n",
    "    sections = []\n",
    "    for idx, content in enumerate(splits[1:]):  # First split is before first header\n",
    "        sec_title = re.sub(r'\\W+', ' ', headers_found[idx]).strip().title()\n",
    "        sec_content = content.strip()\n",
    "        if len(sec_content) > 100:  # Skip empty/very short sections\n",
    "            sections.append((sec_title, sec_content))\n",
    "    return sections\n",
    "\n",
    "# Example: split the sample article\n",
    "sections = split_sections(samples[0]['article'])\n",
    "for t, c in sections:\n",
    "    print(f\"\\n--- {t} ---\\n{c[:200]} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d74f1e",
   "metadata": {},
   "source": [
    "## 4. Summarization (Local T5 Model, GPU Accelerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the model if not already present (first time only, then offline/cached)\n",
    "# You can use \"t5-small\" or \"t5-base\" for better results (or your fine-tuned model)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"t5-small\",   # change to your fine-tuned model if available\n",
    "    tokenizer=\"t5-small\",\n",
    "    device=0 if device==\"cuda\" else -1\n",
    ")\n",
    "\n",
    "def summarize_section(text, max_length=120, min_length=30):\n",
    "    \"\"\"\n",
    "    Summarize a section using T5.\n",
    "    \"\"\"\n",
    "    input_text = \"summarize: \" + text\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "# Summarize the first few sections for the demo\n",
    "summaries = []\n",
    "for section_title, section_text in sections[:4]:\n",
    "    print(f\"\\nSummarizing section: {section_title}\")\n",
    "    summary = summarize_section(section_text[:1500])  # Truncate for demo\n",
    "    summaries.append((section_title, summary))\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc62f2f",
   "metadata": {},
   "source": [
    "## 5. Slide Generation (PowerPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_presentation(summaries, out_path=\"generated_presentation.pptx\"):\n",
    "    prs = Presentation()\n",
    "    title_slide_layout = prs.slide_layouts[0]\n",
    "    content_slide_layout = prs.slide_layouts[1]\n",
    "    \n",
    "    # Title Slide\n",
    "    slide = prs.slides.add_slide(title_slide_layout)\n",
    "    slide.shapes.title.text = \"Automated Slide Generation\"\n",
    "    slide.placeholders[1].text = \"Generated from arXiv Paper Using NLP\"\n",
    "    \n",
    "    # Content Slides\n",
    "    for section_title, summary in summaries:\n",
    "        slide = prs.slides.add_slide(content_slide_layout)\n",
    "        slide.shapes.title.text = section_title\n",
    "        tf = slide.placeholders[1].text_frame\n",
    "        # Split summary into bullet points if possible\n",
    "        bullets = summary.split('. ')\n",
    "        for bullet in bullets:\n",
    "            if bullet.strip():\n",
    "                tf.add_paragraph().text = bullet.strip() + ('.' if not bullet.strip().endswith('.') else '')\n",
    "        # Remove first empty paragraph\n",
    "        if tf.paragraphs and not tf.paragraphs[0].text.strip():\n",
    "            tf._element.remove(tf.paragraphs[0]._element)\n",
    "    prs.save(out_path)\n",
    "    print(f\"\\nPresentation saved as: {out_path}\")\n",
    "\n",
    "# Create the slides!\n",
    "create_presentation(summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e07192",
   "metadata": {},
   "source": [
    "## 6. Full Pipeline Function (Reusable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_article(article_text, n_sections=4):\n",
    "    \"\"\"\n",
    "    Splits, summarizes, and returns sections for slide generation.\n",
    "    \"\"\"\n",
    "    sections = split_sections(article_text)\n",
    "    summaries = []\n",
    "    for section_title, section_text in sections[:n_sections]:\n",
    "        summary = summarize_section(section_text[:1500])\n",
    "        summaries.append((section_title, summary))\n",
    "    return summaries\n",
    "\n",
    "# Example: Full pipeline for the first article\n",
    "demo_summaries = process_article(samples[0]['article'])\n",
    "create_presentation(demo_summaries, out_path=\"demo_presentation.pptx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6263b95",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - This notebook demonstrates a full offline pipeline for extracting, summarizing, and generating PowerPoint slides from research papers.\n",
    "# - It is ready for further customization, batch processing, and improvements.\n",
    "# - For best results, use a domain-specific or fine-tuned summarization model and enhance section splitting heuristics.\n",
    "# - You can easily expand this to process multiple articles, generate fancier slides, or add images from PDF extraction.\n",
    "# - Everything is designed to run offline after model download, and GPU acceleration is enabled.\n",
    "\n",
    "print(\"Notebook pipeline complete! Ready for final project submission or demonstration.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
